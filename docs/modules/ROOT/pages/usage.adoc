= Usage

== Requirements

A distributed Apache HBase installation depends on a running Apache ZooKeeper and HDFS cluster. See
the documentation for the https://docs.stackable.tech/hdfs/usage.html[Stackable Operator for Apache HDFS]
how to set up these clusters.

== Deployment of an Apache HBase cluster

An Apache HBase cluster can be created with the following cluster specification:

[source,yaml]
----
apiVersion: hbase.stackable.tech/v1alpha1
kind: HbaseCluster
metadata:
  name: simple-hbase
spec:
  image:
    productVersion: 2.4.12
    stackableVersion: 0.3.0
  hdfsConfigMapName: simple-hdfs-namenode-default
  zookeeperConfigMapName: simple-hbase-znode
  config:
    hbaseOpts:
    hbaseRootdir: /hbase
  masters:
    roleGroups:
      default:
        replicas: 1
  regionServers:
    roleGroups:
      default:
        replicas: 1
  restServers:
    roleGroups:
      default:
        replicas: 1
---
apiVersion: zookeeper.stackable.tech/v1alpha1
kind: ZookeeperZnode
metadata:
  name: simple-hbase-znode
spec:
  clusterRef:
    name: simple-zk
----

- `hdfsConfigMapName` references the config map created by the Stackable HDFS operator.
- `zookeeperConfigMapName` references the config map created by the Stackable ZooKeeper operator.
- `hbaseOpts` is mapped to the environment variable `HBASE_OPTS` in `hbase-env.sh`.
- `hbaseRootdir` is mapped to `hbase.rootdir` in `hbase-site.xml`.

Please note that the version you need to specify is not only the version of HBase which you want to roll out, but has to be amended with a Stackable version as shown.
This Stackable version is the version of the underlying container image which is used to execute the processes.
For a list of available versions please check our https://repo.stackable.tech/#browse/browse:docker:v2%2Fstackable%2Fhbase%2Ftags[image registry].
It should generally be safe to simply use the latest image version that is available.


== Monitoring

The managed HBase instances are automatically configured to export Prometheus metrics. See
xref:home:operators:monitoring.adoc[] for more details.

== Configuration Overrides

The cluster definition also supports overriding configuration properties and environment variables, either per role or per role group, where the more specific override (role group) has precedence over the less specific one (role).

IMPORTANT: Overriding certain properties which are set by operator can interfere with the operator and can lead to problems.

=== Configuration Properties

For a role or role group, at the same level of `config`, you can specify: `configOverrides` for the following files:

- `hbase-site.xml`
- `hbase-env.sh`

// hdfs-site.xml is not listed here. The file is always taken from the referenced hdfs cluster

For example, if you want to set the `hbase.rest.threads.min` to 4 and the `HBASE_HEAPSIZE` to two GB adapt the `restServers` section of the cluster resource like so:

[source,yaml]
----
restServers:
  roleGroups:
    default:
      config: {}
      configOverrides:
        hbase-site.xml:
          hbase.rest.threads.min: "4"
        hbase-env.sh:
          HBASE_HEAPSIZE: "2G"
      replicas: 1
----

Just as for the `config`, it is possible to specify this at role level as well:

[source,yaml]
----
restServers:
  configOverrides:
    hbase-site.xml:
      hbase.rest.threads.min: "4"
    hbase-env.sh:
      HBASE_HEAPSIZE: "2G"
  roleGroups:
    default:
      config: {}
      replicas: 1
----

All override property values must be strings. The properties will be formatted and escaped correctly into the XML file, respectively inserted as is into the `env.sh` file.

For a full list of configuration options we refer to the HBase https://hbase.apache.org/book.html#config.files[Configuration Documentation].

// Environment configuration is not implemented. The environment is managed
// with the hbase-env.sh configuration file

// CLI overrides are also not implemented

=== Storage for data volumes

The HBase Operator currently does not support any https://kubernetes.io/docs/concepts/storage/persistent-volumes[PersistentVolumeClaims].

=== Resource Requests

// The "nightly" version is needed because the "include" directive searches for
// files in the "stable" version by default.
// TODO: remove the "nightly" version after the next platform release (current: 22.09)
include::nightly@home:concepts:stackable_resource_requests.adoc[]

If no resources are configured explicitly, the HBase operator uses following defaults:

[source,yaml]
----
regionServers:
  roleGroups:
    default:
      config:
        resources:
          cpu:
            min: '200m'
            max: "4"
          memory:
            limit: '2Gi'
----

WARNING: The default values are _most likely_ not sufficient to run a proper cluster in production. Please adapt according to your requirements.

For more details regarding Kubernetes CPU limits see: https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[Assign CPU Resources to Containers and Pods].

== Phoenix

The Apache Phoenix project provides the ability to interact with HBase with JBDC using familiar SQL-syntax. The Phoenix dependencies are bundled with the Stackable HBase image and do not need to be installed separately (client components will need to ensure that they have the correct client-side libraries available). Information about client-side installation can be found https://phoenix.apache.org/installation.html[here].

Phoenix comes bundled with a few simple scripts to verify a correct server-side installation. For example, assuming that phoenix dependencies have been installed to their default location of `/stackable/phoenix/bin`, we can issue the following using the supplied `psql.py` script:

[source,shell script]
----
/stackable/phoenix/bin/psql.py  && \
   /stackable/phoenix/examples/WEB_STAT.sql && \
   /stackable/phoenix/examples/WEB_STAT.csv  && \
   /stackable/phoenix/examples/WEB_STAT_QUERIES.sql
----

This script creates a java command that creates, populates and queries a Phoenix table called `WEB_STAT`. Alternatively, one can use the `sqlline.py` script (which wraps the https://github.com/julianhyde/sqlline[sqlline] utility):

[source,shell script]
----
/stackable/phoenix/bin/sqlline.py [zookeeper] [sql file]
----

The script opens an SQL prompt from where one can list, query, create and generally interact with Phoenix tables. So, to query the table that was created in the previous step, start the script and enter some SQL at the prompt:

image::phoenix_sqlline.png[Phoenix Sqlline]

The Phoenix table `WEB_STAT` is created as an HBase table, and can be viewed normally from within the HBase UI:

image::phoenix_tables.png[Phoenix Tables]

The `SYSTEM`* tables are those required by Phoenix and are created the first time that Phoenix is invoked.

NOTE: Both `psql.py` and `sqlline.py` generate a java command that calls classes from the Phoenix client library `.jar`. The Zookeeper quorum does not need to be supplied as part of the URL used by the JDBC connection string, as long as the environment variable `HBASE_CONF_DIR` is set and supplied as an element for the `-cp` classpath search: the cluster information is then extracted from `$HBASE_CONF_DIR/hbase-site.xml`.

